1. 성능 최적화
- mac m1에서 gemma3:1b llm응답시 6-12초 정도 걸림 
  - 최우선적으로는 퍼포먼스 튜닝이 필요
  - 리서치 결과 해당 모델이 최선인의 초이스인 것 같으나, 여전히 느린 응답 
  - ✅ Redis 캐시를 통한 응답 시간 개선
  - ✅ 임베딩 기반 유사 답변 캐싱 구현

- ✅ 실제 oci 무료 서버에 배포할 경우를 고려한 캐싱 전략 구현
  - ✅ 서술형 답변에 대한 캐싱 전략 구현:
    - ✅ 유사도 기반 캐싱
    - ✅ Hash 기반 다중 답변 저장
    - ✅ 설정 가능한 만료 시간
  - 도커 컨테이너로 저장 필요  
     - 캐시를 어떻게 추가할 지에 대해서는 고민 필요 
     - job queue를 별도로 이용해 비동기로 캐시에 데이터를 저장할 지..?
     - oci 무료 서버임을 감안할 필요 있음

2. 데이터 스키마 구조 최적화 및 변경
- ✅ 서술형 답변 구조 구현
- 실제 질문과 유사하게 서술형 구조로 변경 필요 
- 다음 문제 추천 시스템:
  - 질문 키워드 기반 유사도 계산
  - 사용자의 이전 답변 패턴 분석
  - 난이도 기반 적응형 문제 제시
- migrate schema, gorm model, repository, api response 수정 필요 

3. ✅ 도메인 에러 구조 추가 및 에러 코드 응답 추가
- ✅ 도메인 중심 에러 구조 구현
- ✅ 에러 미들웨어 추가
- ✅ 적절한 HTTP 상태 코드 매핑
- ✅ 상세 에러 메시지 제공

4. 아키텍처 개선
- ✅ Port & Adapter 패턴 적용
- ✅ Clean Architecture 원칙 준수
- ✅ 도메인 주도 설계(DDD) 적용
- ✅ 캐시 계층 추상화

5. 질문 추가 배치 작업
- 상용 llm(gemmini api)를 이용해서 질문 데이터 추가
  - 이 때 미리 0~ 0.3 0.3~0.6 0.6~0.9 답변도 미리 만들어서 데이터를 저장하고 주기적으로 배치 작업을 통해 질문, 카테고리 추가

6. 새로운 기능 제안
- 사용자 답변 패턴 분석
- 개인화된 학습 경로 제시
- 실시간 피드백 시스템
- 성능 메트릭 수집 및 분석
- API 버저닝 시스템 구현
